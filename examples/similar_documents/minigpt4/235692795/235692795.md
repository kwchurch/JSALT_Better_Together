<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719">131: VinVL: Revisiting Visual Representations in Vision-Language Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.990</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/be0014c1fbc3e664686610d2c85f75038a4f6e4f">52: VinVL: Making Visual Representations Matter in Vision-Language Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.983</td>
<td>-0.017</td>
<td>NA:232183432</td>
</tr>
<tr>
<td>Specter</td>
<td>0.829</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/642c24d83764dccb0426c5ffd03b12e20a2817c3">0: VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.829</td>
<td>0.948</td>
<td><a href="https://www.semanticscholar.org/paper/cd71c96e05068b26e8f83e6c61a6a239685e943a">7: Flamingo: a Visual Language Model for Few-Shot Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.818</td>
<td>0.990</td>
<td><a href="https://www.semanticscholar.org/paper/6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9">214: 12-in-1: Multi-Task Vision and Language Representation Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.813</td>
<td>0.765</td>
<td><a href="https://www.semanticscholar.org/paper/560938251d3a5d844222c4bbee7c7623cc2d1cd8">2: Leveraging Auxiliary Text for Deep Recognition of Unseen Visual Relationships</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.812</td>
<td>-0.017</td>
<td><a href="https://www.semanticscholar.org/paper/52e66cf8a6ec5ad2c6e392f6e08d7783cce21600">0: Vision and language: from visual question answering to domain-invariant representations</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.804</td>
<td>0.790</td>
<td><a href="https://www.semanticscholar.org/paper/3ac52540b66e6e57e78bcdc48d17c5797bcb32fc">0: Vision-Language Pre-Training for Boosting Scene Text Detectors</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.802</td>
<td>-0.017</td>
<td><a href="https://www.semanticscholar.org/paper/02f2622ae14dba1a4f4e58d5b4ecf2a08a8bcaa0">0: How visual salience influences natural language descriptions</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.799</td>
<td>0.494</td>
<td><a href="https://www.semanticscholar.org/paper/9a41131a159c82df50636bc694e1ec59dc350bec">3: Reasonable Perception: Connecting Vision and Language Systems for Validating Scene Descriptions</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.796</td>
<td>0.911</td>
<td><a href="https://www.semanticscholar.org/paper/cb56ea2d4de29481e25df6c318afc217eb7e4a7d">10: LanguageRefer: Spatial-Language Model for 3D Visual Grounding</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.794</td>
<td>0.790</td>
<td><a href="https://www.semanticscholar.org/paper/e4434b3715c0f15fbeba2ac0bd82cdea11f72800">0: VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.792</td>
<td>-0.017</td>
<td><a href="https://www.semanticscholar.org/paper/860e897c86be1f12336640f37150bddc38653e1f">0: Learning to navigate by distilling visual information and natural language instructions</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.792</td>
<td>0.637</td>
<td><a href="https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0">4457: Show and tell: A neural image caption generator</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.790</td>
<td>0.951</td>
<td><a href="https://www.semanticscholar.org/paper/507ebc03b4cb58f278969f78a1cb49403a9cabf8">2: Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.789</td>
<td>0.975</td>
<td><a href="https://www.semanticscholar.org/paper/145e8a3cad5a1d4767c72b239b776fda7a682eb8">1: CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.788</td>
<td>0.480</td>
<td><a href="https://www.semanticscholar.org/paper/0026d112cf8f3b98e45455d967de9ca3c33d22f6">12: Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.787</td>
<td>0.383</td>
<td><a href="https://www.semanticscholar.org/paper/c19f8380410181249fdca70ec14f6b5f38ae0846">1: Exploring Language Prior for Mode-Sensitive Visual Attention Modeling</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.785</td>
<td>0.857</td>
<td><a href="https://www.semanticscholar.org/paper/3e8d5d3e74d5f61d63fc74f86a238765df2ebb05">1: Ripple Attention for Visual Perception with Sub-quadratic Complexity</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719">131: VinVL: Revisiting Visual Representations in Vision-Language Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.990</td>
<td>0.997</td>
<td><a href="https://www.semanticscholar.org/paper/be0014c1fbc3e664686610d2c85f75038a4f6e4f">52: VinVL: Making Visual Representations Matter in Vision-Language Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.818</td>
<td>0.996</td>
<td><a href="https://www.semanticscholar.org/paper/997a8f96d4fa6f7239de2be08a8a1e95479571d4">4: KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.820</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/c05cd00ae61f3c1c39be2603a2f96fdfe0c59dd8">5: UFO: A UniFied TransfOrmer for Vision-Language Representation Learning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.749</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/57157cf2547d58be55e05b689435c7915faced1d">0: HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.809</td>
<td>0.995</td>
<td><a href="https://www.semanticscholar.org/paper/818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57">483: Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.811</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/2fa4938001b18f464c62aa38a5a469bb92569d57">57: Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.848</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/f5aaf9f38ba6e00f1645beab6b83176452322cef">0: PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.799</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/d5f3cb388933db53d5391d6cd462e8499facee78">7: Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.798</td>
<td>0.994</td>
<td><a href="https://www.semanticscholar.org/paper/bc996a4dbf9d4234eacdd0b930a94de1d158e256">119: ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.774</td>
<td>0.993</td>
<td><a href="https://www.semanticscholar.org/paper/394be105b87e9bfe72c20efe6338de10604e1a11">74: Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.803</td>
<td>0.993</td>
<td><a href="https://www.semanticscholar.org/paper/d2b50f9d653e751fd51202ef08fc4db8089c3a9f">1: Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.823</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/fea02a76f504f6dfefd2497220da913c5274f5ab">21: E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.728</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/2d626d4f57844a93431fba2a533f497ebefae041">1: Answer-Me: Multi-Task Open-Vocabulary Visual Question Answering</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.790</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/97cef55f14b227e53dba94fdad2149962843044e">9: SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.659</td>
<td>0.992</td>
<td><a href="https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0">593: Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.800</td>
<td>0.991</td>
<td><a href="https://www.semanticscholar.org/paper/458af0f3f03229290572a2630c75ac56e9dbec6e">19: CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.743</td>
<td>0.991</td>
<td><a href="https://www.semanticscholar.org/paper/5c09c7b9d749e7a1f90573b0cfd53606f1038d73">10: Probing Image-Language Transformers for Verb Understanding</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.837</td>
<td>0.991</td>
<td><a href="https://www.semanticscholar.org/paper/6648b4db5f12c30941ea78c695e77aded19672bb">330: Unified Vision-Language Pre-Training for Image Captioning and VQA</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.706</td>
<td>0.990</td>
<td><a href="https://www.semanticscholar.org/paper/598a2ee223e2949c3b28389e922c1892b4717d2a">133: Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</a></td>
</tr>
</table></html>
