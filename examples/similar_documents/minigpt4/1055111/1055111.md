<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd">7157: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.774</td>
<td>0.210</td>
<td><a href="https://www.semanticscholar.org/paper/41ec7222ad2583ae88ee2f91f4e7c2e98432765f">0: OOSTING SALIENCY PREDICTION WITH FEATURE MAPS TRAINED ON I MAGE N ET</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.774</td>
<td>0.081</td>
<td><a href="https://www.semanticscholar.org/paper/f0d75c37e233875f2de4e5a071f6660be766c6d0">2: Remember What You have drawn: Semantic Image Manipulation with Memory</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.769</td>
<td>-0.031</td>
<td><a href="https://www.semanticscholar.org/paper/634659fd051d8d9cf6b84d6b7171388608b48227">0: Unsupervised Learning for Object Representations by Watching and Moving</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.768</td>
<td>0.244</td>
<td><a href="https://www.semanticscholar.org/paper/0026d112cf8f3b98e45455d967de9ca3c33d22f6">12: Online Bag-of-Visual-Words Generation for Unsupervised Representation Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.767</td>
<td>0.099</td>
<td><a href="https://www.semanticscholar.org/paper/a10d6877c90de39c42a143af60c0bf5e588be763">238: Deep Visual Analogy-Making</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.767</td>
<td>0.068</td>
<td><a href="https://www.semanticscholar.org/paper/dc55fa2fa5adfe2847117db1c4364781597fd815">6: Meaning maps and saliency models based on deep convolutional neural networks are insensitive to image meaning when predicting human fixations</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.763</td>
<td>0.086</td>
<td><a href="https://www.semanticscholar.org/paper/ff2bb372b6a419d274284379f2ce309cd4f6d425">9: Meaning maps and saliency models based on deep convolutional neural networks are insensitive to image meaning when predicting human fixations</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.756</td>
<td>0.020</td>
<td><a href="https://www.semanticscholar.org/paper/768d483ed5879d94a413bd95b275438b8b19fb0d">3: Selective Attention in the Learning of Invariant Representation of Objects</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.749</td>
<td>0.609</td>
<td><a href="https://www.semanticscholar.org/paper/b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1">83: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.747</td>
<td>0.659</td>
<td><a href="https://www.semanticscholar.org/paper/9dcaf5ab101ba551ac334f3ede177a444e154643">6: Referring Transformer: A One-step Approach to Multi-task Visual Grounding</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.744</td>
<td>0.241</td>
<td><a href="https://www.semanticscholar.org/paper/dc73e4bc71ec6e981d08fcb69952bebd9e5e538a">3: Learning Visual Representations for Transfer Learning by Suppressing Texture</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.744</td>
<td>0.764</td>
<td><a href="https://www.semanticscholar.org/paper/007ca8ca7a68451c32da034c72a06238434843c1">5: Learning to Learn Words from Visual Scenes</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.743</td>
<td>-0.031</td>
<td>NA:248392415</td>
</tr>
<tr>
<td>Specter</td>
<td>0.743</td>
<td>0.891</td>
<td><a href="https://www.semanticscholar.org/paper/bb48b7b39d86cd5156cda58636088c702be10c50">64: Summarizing Videos with Attention</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.741</td>
<td>0.021</td>
<td><a href="https://www.semanticscholar.org/paper/ef19a772b228fe6acd10d257b59371de379b781b">10: Learning to Detect Salient Objects in Natural Scenes Using Visual Attention</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.739</td>
<td>0.752</td>
<td><a href="https://www.semanticscholar.org/paper/574cfdc454a6b44026fcbc5539127ca507ca3045">7: RefVOS: A Closer Look at Referring Expressions for Video Object Segmentation</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.737</td>
<td>0.698</td>
<td><a href="https://www.semanticscholar.org/paper/cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c">38: What Does BERT with Vision Look At?</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.736</td>
<td>0.590</td>
<td><a href="https://www.semanticscholar.org/paper/4814c10f84863e016d75e6af42e790f60759b9f4">51: Compositional Obverter Communication Learning From Raw Visual Input</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.731</td>
<td>0.872</td>
<td><a href="https://www.semanticscholar.org/paper/6834deeabfbc3ca5989ca7a3bd76dc5a84c5a346">12: Erasing-based Attention Learning for Visual Question Answering</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd">7157: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.826</td>
<td>0.980</td>
<td><a href="https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0">4457: Show and tell: A neural image caption generator</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.619</td>
<td>0.975</td>
<td><a href="https://www.semanticscholar.org/paper/076b02b481a41f1e07b8a2bdbe0ac8d946f9872e">9: Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.729</td>
<td>0.973</td>
<td><a href="https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88">2464: Deep Visual-Semantic Alignments for Generating Image Descriptions</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.653</td>
<td>0.973</td>
<td><a href="https://www.semanticscholar.org/paper/a1af6068ea47b37648ffe0075242c92a39dfabf8">0: Component based comparative analysis of each module in image captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.679</td>
<td>0.971</td>
<td><a href="https://www.semanticscholar.org/paper/08903ceeee6420992d30ff3f3b8b4830118af4d9">245: Attention-Based Multimodal Fusion for Video Description</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.557</td>
<td>0.969</td>
<td><a href="https://www.semanticscholar.org/paper/441dc546f4658852779319b50bdee739d78485df">3: Unifying Relational Sentence Generation and Retrieval for Medical Image Report Composition</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.446</td>
<td>0.968</td>
<td><a href="https://www.semanticscholar.org/paper/0da353e79f666a3ae7dd0a5d28c75b852a7f60bf">344: SHOW</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.825</td>
<td>0.968</td>
<td><a href="https://www.semanticscholar.org/paper/62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e">615: Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.700</td>
<td>0.967</td>
<td><a href="https://www.semanticscholar.org/paper/b196bc11ad516c8e6ff96f83acfc443fd7161730">217: ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.503</td>
<td>0.965</td>
<td><a href="https://www.semanticscholar.org/paper/65a01b760850d82505c2a04faf84a3e8c50398fe">0: A Review on Methods and Applications in Multimodal Deep Learning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.603</td>
<td>0.962</td>
<td><a href="https://www.semanticscholar.org/paper/9fbdb53c100005ac890989beb3d78e208ba9acda">9: Retrieval Topic Recurrent Memory Network for Remote Sensing Image Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.735</td>
<td>0.961</td>
<td><a href="https://www.semanticscholar.org/paper/3bb4f2013d99eaf2afc182fa482bd0f2d63f2d82">64: Visual Question Answering with Memory-Augmented Networks</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.717</td>
<td>0.961</td>
<td><a href="https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3">145: Hierarchical LSTMs with Adaptive Attention for Visual Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.742</td>
<td>0.960</td>
<td><a href="https://www.semanticscholar.org/paper/799537fa855caf53a6a3a7cf20301a81e90da127">64: High-Order Attention Models for Visual Question Answering</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.683</td>
<td>0.959</td>
<td><a href="https://www.semanticscholar.org/paper/a5d8c57c53d896275d6fa2d1137cd152a2cd7624">36: Image Captioning at Will: A Versatile Scheme for Effectively Injecting Sentiments into Image Descriptions</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.661</td>
<td>0.958</td>
<td><a href="https://www.semanticscholar.org/paper/230a8581672b3147238eaab2cf686c70fe4f672b">7: Convolutional Reconstruction-to-Sequence for Video Captioning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.636</td>
<td>0.958</td>
<td><a href="https://www.semanticscholar.org/paper/b77e2c41c3af03a1fd07595f41a7c191d7897962">26: Look, listen, and decode: Multimodal speech recognition with images</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.672</td>
<td>0.958</td>
<td><a href="https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4">1089: Sequence to Sequence -- Video to Text</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.669</td>
<td>0.958</td>
<td><a href="https://www.semanticscholar.org/paper/da958d2604e9f86f94a441d60488d0e93451c248">42: Captioning Transformer with Stacked Attention Modules</a></td>
</tr>
</table></html>
