<html><table><tr>
<th>Method</th>
<th>cosS</th>
<th>cosP</th>
<th>paper</th>
</tr>
<tr>
<td>Specter</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/6b85b63579a916f705a8e10a49bd8d849d91b1fc">4191: Language Models are Few-Shot Learners</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.903</td>
<td>0.971</td>
<td><a href="https://www.semanticscholar.org/paper/85e7d63f75c0916bd350a229e040c5fbb1472e7a">295: Making Pre-trained Language Models Better Few-shot Learners</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.862</td>
<td>0.978</td>
<td><a href="https://www.semanticscholar.org/paper/b58d8579ece27a60432e667bfbdb750590fa65d9">78: True Few-Shot Learning with Language Models</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.857</td>
<td>0.958</td>
<td><a href="https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd">119: Finetuned Language Models Are Zero-Shot Learners</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.848</td>
<td>0.843</td>
<td><a href="https://www.semanticscholar.org/paper/56774a88a1cd216a50e2ec9d26fe3c73f6d4d601">4: Improving Few-shot Text Classification via Pretrained Language Representations</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.842</td>
<td>0.700</td>
<td><a href="https://www.semanticscholar.org/paper/75344c9423d0abf65391d36464efb4ed177d01ce">0: Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.840</td>
<td>0.923</td>
<td><a href="https://www.semanticscholar.org/paper/6263f69b0154fb090fc76f792c1a4aa182e24294">2: Language Models are Few-Shot Butlers</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.838</td>
<td>0.967</td>
<td><a href="https://www.semanticscholar.org/paper/605c32428861eb26b8631617b8f6c97a850d6a04">4: True Few-Shot Learning with Prompts -- A Real-World Perspective</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.837</td>
<td>0.953</td>
<td><a href="https://www.semanticscholar.org/paper/e1227daa4877599e13de41a5207a222e1b197456">4: RAFT: A Real-World Few-Shot Text Classification Benchmark</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.835</td>
<td>0.977</td>
<td><a href="https://www.semanticscholar.org/paper/e54ffc76d805c48660bb0fd20019ca82ac94ba0d">35: Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.831</td>
<td>0.882</td>
<td><a href="https://www.semanticscholar.org/paper/89b1223ba8a778b8a31fa3e9de22879081489bb1">5: Few-Shot Learning with Siamese Networks and Label Tuning</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.830</td>
<td>0.850</td>
<td><a href="https://www.semanticscholar.org/paper/d393943a873ead524069d0f7f55acef05cc9ba45">36: Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.829</td>
<td>0.914</td>
<td><a href="https://www.semanticscholar.org/paper/1c81225b4146297173e034a03fcb4c74b4e52e0f">0: Active Few-Shot Learning with FASL</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.828</td>
<td>0.444</td>
<td><a href="https://www.semanticscholar.org/paper/93d73c427af2527f96199ecfa52d1d652f936552">1: Cross-domain Few-shot Learning with Task-specific Adapters</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.827</td>
<td>0.958</td>
<td><a href="https://www.semanticscholar.org/paper/c28b7dfe341f1e13a5a98efbce7946ef795cf9b8">21: SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.822</td>
<td>0.946</td>
<td><a href="https://www.semanticscholar.org/paper/32881086e4f0d97a81b0f9c89b59384040b1d02d">0: LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.817</td>
<td>0.971</td>
<td><a href="https://www.semanticscholar.org/paper/cc50f846ed7222698d130cddbc58ed4d547914ed">38: CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.815</td>
<td>0.872</td>
<td><a href="https://www.semanticscholar.org/paper/4ef19969c930705012bfd0f6c74bc4ff3020bfe2">7: Few-shot Sequence Learning with Transformers</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.814</td>
<td>0.943</td>
<td><a href="https://www.semanticscholar.org/paper/16bf88a6d172699cb9a26a6936efb4941e3f3c13">1: An Application of Pseudo-Log-Likelihoods to Natural Language Scoring</a></td>
</tr>
<tr>
<td>Specter</td>
<td>0.814</td>
<td>0.686</td>
<td><a href="https://www.semanticscholar.org/paper/be0fbb810583930c071d0b9b2c5187fe260783f5">36: Swin Transformer V2: Scaling Up Capacity and Resolution</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>1.000</td>
<td>1.000</td>
<td><a href="https://www.semanticscholar.org/paper/6b85b63579a916f705a8e10a49bd8d849d91b1fc">4191: Language Models are Few-Shot Learners</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.825</td>
<td>0.991</td>
<td><a href="https://www.semanticscholar.org/paper/29ddc1f43f28af7c846515e32cc167bc66886d0c">515: Parameter-Efficient Transfer Learning for NLP</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.746</td>
<td>0.988</td>
<td><a href="https://www.semanticscholar.org/paper/464026e25beabb637bb3087e1243613715351a52">14: MRD-Net: Multi-Modal Residual Knowledge Distillation for Spoken Question Answering</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.730</td>
<td>0.987</td>
<td><a href="https://www.semanticscholar.org/paper/08c6c2f51cbd8b9621909c24555c7e510babf2f1">19: Knowledge Distillation for Improved Accuracy in Spoken Question Answering</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.727</td>
<td>0.987</td>
<td><a href="https://www.semanticscholar.org/paper/d22e4cc3a501c17881b9478621f29760e429e76e">41: Parameter-Efficient Transfer Learning with Diff Pruning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.698</td>
<td>0.987</td>
<td><a href="https://www.semanticscholar.org/paper/3fd0f34117cf9395130e08c3f02ac2dadcca7206">325: Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.573</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/4f68e07c6c3173480053fd52391851d6f80d651b">233: On the Opportunities and Risks of Foundation Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.692</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/e6c561d02500b2596a230b341a8eb8b921ca5bf2">450: Scaling Laws for Neural Language Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.777</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/76c9558b3fa10baf0e094386a650015b29a8a4bc">2: Compositional generalization in semantic parsing with pretrained transformers</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.804</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/32ead456a7316d26893861169f57ddaeb512dabb">2: CTR-BERT: Cost-effective knowledge distillation for billion-parameter teacher models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.706</td>
<td>0.986</td>
<td><a href="https://www.semanticscholar.org/paper/de8f92c8a7ebde8bffb968a536f79e5fb7cd225e">5: Scaling Laws for Acoustic Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>-1.000</td>
<td>0.985</td>
<td><a href="https://www.semanticscholar.org/paper/43f2ad297941db230c089ba353efc3f281ab678c">1056: 5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.786</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/7ebed46b7f3ec913e508e6468304fcaea832eda1">102: Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.767</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/09eb94aec09ccefdfbb2621e5085696c3b6a267e">4: Technical Report: Auxiliary Tuning and its Application to Conditional Text Generation</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.813</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/6bd91a3183ddb844641acb9f3fe9faec6a9ff617">8: Meta-learning via Language Model In-context Tuning</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.805</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/8b9d77d5e52a70af37451d3db3d32781b83ea054">117: On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.707</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/cbf3bf8f541f5b446c59c8deacbcc18527768c75">0: M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.813</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/7402b604f14b8b91c53ed6eed04af92c59636c97">236: Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.822</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/0cbf97173391b0430140117027edcaf1a37968c7">592: TinyBERT: Distilling BERT for Natural Language Understanding</a></td>
</tr>
<tr>
<td>Proposed</td>
<td>0.728</td>
<td>0.984</td>
<td><a href="https://www.semanticscholar.org/paper/b832638a7c8f8e9b51b2762e36fbd29733af63b2">2: Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space</a></td>
</tr>
</table></html>
